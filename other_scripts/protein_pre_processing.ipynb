{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#miss forest doesn't work for current version of sklearn\n",
    "#if necessary, downgrade sklearn version\n",
    "#https://github.com/scikit-learn/scikit-learn/discussions/25745\n",
    "\n",
    "#pip uninstall scikit-learn -y\n",
    "#pip install scikit-learn==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.4\n"
     ]
    }
   ],
   "source": [
    "import sspa\n",
    "import scipy\n",
    "print(sspa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#ignores future warnings for miss forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl   #to read in a sheet of the excel file\n",
    "\n",
    "#imputation libraries  (import sklearn before importing sys and missingpy otherwise an error will happen)\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#imputation libraries (read this, then the cell below, then this cell again)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39msklearn.neighbors.base\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39;49mneighbors\u001b[39m.\u001b[39m_base\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'neighbors'"
     ]
    }
   ],
   "source": [
    "#imputation libraries (read this, then the cell below, then this cell again)\n",
    "import sys\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#imputation libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmissingpy\u001b[39;00m \u001b[39mimport\u001b[39;00m MissForest\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'missingpy'"
     ]
    }
   ],
   "source": [
    "#imputation libraries\n",
    "from missingpy import MissForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded for the plasma proteomic dataset from the following paper: <br>\n",
    "Multi-Omics Resolves a Sharp Disease-State Shift between Mild and Moderate COVID-19 (Su et al., 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../Data/Su_raw.xlsx', sheet_name=3)   #Sheet name = 3, proteomic     (sheet name = 4, metabolomic)\n",
    "\n",
    "controls = data.loc[data['Group'] == 'Healthy Donor ']\n",
    "cases = data.loc[data['Group'] == 'COVID19 ']\n",
    "\n",
    "#There are two readings for each patient e.g. INCOV051-BL and INCOV051-AC\n",
    "#BL stands for the first draw (this is the one we want); AC is the second draw and can be filtered out\n",
    "first_draws = cases[cases.loc[:,'sample_id'].str.contains(\"BL\")]\n",
    "first_draws.loc[:,'sample_id'] = first_draws.loc[:,'sample_id'].str.rstrip('-BL')  #remove 'BL' label from the cases (so I can match to WHO status)\n",
    "\n",
    "df = pd.concat([controls, first_draws], ignore_index=True)  #concatenate dataset and reset index \n",
    "df.insert(0, \"WHO_status\", '0') #insert new column for WHO_status and temporarily add all zeros\n",
    "\n",
    "#display(df['sample_id'][124:130])    #first instance of a case is at index 124\n",
    "display(df.iloc[:,3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in WHO status data\n",
    "who_data = pd.read_excel('../Data/Su_raw.xlsx', sheet_name=1)\n",
    "\n",
    "#Subset to only first draw readings\n",
    "who_data = who_data[who_data[\"Sample ID\"].str.contains(\"-1\")]\n",
    "who_data = who_data.reset_index(drop=True) #Reset index and drop old index\n",
    "display(who_data)\n",
    "\n",
    "\n",
    "#Create a dictionary with the sample names as a key and WHO status as a value\n",
    "who_dict = {}\n",
    "\n",
    "for i in range(0,len(who_data.index)):\n",
    "    who = who_data[\"Who Ordinal Scale\"][i]\n",
    "    if who == 1 or who == 2 or who == \"1 or 2\":\n",
    "        who = '1-2'\n",
    "    elif who == 3 or who == 4:\n",
    "        who = '3-4'\n",
    "    elif who == 5 or who == 6 or who == 7:\n",
    "        who = '5-7'\n",
    "    else:\n",
    "        print(who)\n",
    "        who = \"ERROR\"\n",
    "\n",
    "    sample = who_data[\"Study Subject ID\"][i]\n",
    "    who_dict[sample] = who\n",
    "\n",
    "\n",
    "\n",
    "print(who_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add WHO data into the original dataframe\n",
    "for i in range(124,len(df.index)): #first instance of a case is at index 124\n",
    "    sample = df.loc[i,\"sample_id\"]\n",
    "    if sample in who_dict:\n",
    "        df.loc[i,\"WHO_status\"] = who_dict[sample]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a different dataframe containing the meta data only (this will be changed later when samples are removed)\n",
    "df.set_index('sample_id', inplace=True)\n",
    "df.columns.values.tolist()\n",
    "\n",
    "meta_data = df.iloc[:,[0,1]]\n",
    "display(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num  = df.iloc[:,2:] #all rows, all columns apart from the first three which is meta data\n",
    "df_num = df_num.set_index('sample_id')\n",
    "\n",
    "#Percentage of missing data in dataframe\n",
    "missing_vals = df_num.isna().sum().sum()\n",
    "print((missing_vals/(259*464))*100) #1.6% missing data for proteomic\n",
    "#print((missing_vals/(266*1050))*100) #19.3% for metabolomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify which columns have missing data\n",
    "#print(df_num.isna().any()[lambda x: x])\n",
    "\n",
    "#Remove features with >40% missing data - same as Sara's thesis\n",
    "df_filter = df_num.loc[:, df_num.isin([' ', np.nan, 0]).mean() <= 0.4]    #keep all rows, if columns have ' ' or nan or 0, count as though missing and take as TRUE, get the mean of all the TRUE values\n",
    "print(df_filter.isna().any()[lambda x: x]) #Identify which columns still have missing data\n",
    "\n",
    "\n",
    "#For metabolomic 895 metabolites left after filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Identify which rows have missing data\n",
    "#print(df_filter[df_filter.isna().any(axis=1)])\n",
    "\n",
    "#identify which ROWS have missing values for each sample\n",
    "df_missing_row = df_filter[df_filter.isna().any(axis=1)]\n",
    "for index, row in df_missing_row.iterrows():\n",
    "    for column in df_missing_row.columns:\n",
    "        if pd.isnull(row[column]):\n",
    "            print(index,column)\n",
    "\n",
    "#Remove samples with >50% missing data - same as Sara's thesis (none of the samples were removed)\n",
    "df_filter = df_filter.loc[df_filter.isin([' ', np.nan, 0]).mean(axis=1) <= 0.5,:]    \n",
    "\n",
    "missing_vals = df_filter.isna().sum().sum()    #.sum() takes sum of missing values in each column, so .sum() again to take sum of all column values\n",
    "print((missing_vals/(259*454))*100)  #259 rows, 454 proteins\n",
    "\n",
    "display(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing data (ignore the warning messages)\n",
    "imputer = MissForest()\n",
    "X_imputed = imputer.fit_transform(df_filter.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check imputed data to make sure there are no missing values\n",
    "df_impute = pd.DataFrame(X_imputed,columns=df_filter.columns, index=df_filter.index)\n",
    "missing_vals = df_impute.isna().sum().sum()\n",
    "print((missing_vals/(259*464))*100) #no missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data has already gone through log2normalisation (negative values present frmo values < 1)\n",
    "#Taking log2 of a number under 1 will give a negative value\n",
    "#Note: log2 transforming is a normalisation step, which usually reduces the skew in the data, so that the data can be scaled using StandardScaler()\n",
    "print(df_impute.max().max())\n",
    "print(df_impute.min().min())\n",
    "print(df_impute.mean(axis = 0)) #if scaled -> mean of 0   (not scaled yet)\n",
    "print(df_impute.std(axis = 0)) #if scaled -> sd of 1       (not scaled yet)\n",
    "\n",
    "display(df_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_impute.mean(axis = 0)\n",
    "sns.histplot(df_hist, bins = 30,color='#BABAE2',edgecolor=\"k\") \n",
    "\n",
    "#The mean value for each metabolite has been plotted\n",
    "plt.title('Mean protein distribution before scaling',fontsize=16)\n",
    "plt.xlabel('Protein expression',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) \n",
    "\n",
    "#plt.savefig( 'Figures/prescale_mean_protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_np = df_impute.to_numpy()\n",
    "df_hist = df_impute_np.flatten()\n",
    "sns.histplot(df_hist, bins = 30,color='#BABAE2',edgecolor=\"k\") \n",
    "\n",
    "#The mean value for each metabolite has been plotted\n",
    "plt.title('Protein distribution before scaling',fontsize=16)\n",
    "plt.xlabel('Protein expression',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) \n",
    "\n",
    "#plt.savefig( 'Figures/prescale_protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I decided not to use because too stringent (>20 outliers detected)\n",
    "\n",
    "# Carry out PCA to identify outliers  \n",
    "#https://towardsdatascience.com/outlier-detection-using-principal-component-analysis-and-hotellings-t2-and-spe-dmodx-methods-625b3c90897\n",
    "\n",
    "#from pca import pca\n",
    "\n",
    "# Initialize pca to also detected outliers\n",
    "#model = pca(normalize=True, detect_outliers=['ht2', 'spe'], n_std=2  )\n",
    "\n",
    "# Fit and transform\n",
    "#results = model.fit_transform(df_impute)\n",
    "\n",
    "# Get the outliers using Hotellings T2 method\n",
    "#df_impute.loc[results['outliers']['y_bool'], :]\n",
    "\n",
    "# Get the outliers using SPE/DmodX method\n",
    "#df_impute.loc[results['outliers']['y_bool_spe'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct PCA to identify outliers\n",
    "\n",
    "features = df_impute.columns[:]\n",
    "x = df_impute.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=20) \n",
    "principal_components = pca.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use elbow method for scree plot to determine number of PCs to use to calculate residuals (4 principal components)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.lineplot(x=PC_values, y=pca.explained_variance_ratio_,  color='#42A7A9',marker=\"o\")   # marker=\"o\" shows markers as dots\n",
    "plt.title('Scree Plot',fontsize=20)\n",
    "plt.xlabel('Principal Component Number',fontsize=15)\n",
    "plt.ylabel('Variance Explained',fontsize=15)\n",
    "\n",
    "#plt.savefig( 'Figures/Scree_plot_protein_QC.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual residual variance method to identify outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct PCA to identify outliers\n",
    "\n",
    "features = df_impute.columns[:]\n",
    "x = df_impute.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=4) #determined using scree plot in the above cell\n",
    "principal_components = pca.fit_transform(x)\n",
    "\n",
    "\n",
    "\n",
    "#Calculate residuals to identify outliers\n",
    "#Formula is:     Residuals = Data matrix - (PCA scores * PCA loadings)  https://datascience.stackexchange.com/questions/29132/how-can-i-extract-the-residual-array-from-the-scikit-learn-pca-routine\n",
    "\n",
    "#Obtain PCA loadings (https://stackoverflow.com/questions/21217710/factor-loadings-using-sklearn)\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "residuals = df_impute - principal_components.dot(loadings.transpose())\n",
    "display(residuals)\n",
    "\n",
    "#Calculate individual residual variance by taking the Euclidean norm (L2 norm) of values for each sample\n",
    "#https://stackoverflow.com/questions/7741878/how-to-apply-numpy-linalg-norm-to-each-row-of-a-matrix\n",
    "l2_norm = pd.DataFrame(np.linalg.norm(residuals,axis=1),columns = ['Individual residual variance'], index = df.index)\n",
    "l2_norm_unordered = l2_norm.copy()\n",
    "l2_norm = l2_norm.sort_values(['Individual residual variance'], ascending=[False])\n",
    "display(l2_norm[:10])\n",
    "\n",
    "#Top 10 outliers\n",
    "outliers = list(l2_norm[:10].index.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for bar graph \n",
    "l2_norm_unordered['Sample ID'] = l2_norm_unordered.index\n",
    "l2_norm_unordered = pd.concat([l2_norm_unordered, meta_data], axis = 1)\n",
    "l2_mean = l2_norm_unordered['Individual residual variance'].mean()\n",
    "l2_std = l2_norm_unordered['Individual residual variance'].std()\n",
    "\n",
    "\n",
    "#Bar graph showing individual residual variance with mean/median line\n",
    "g = sns.barplot(data=l2_norm_unordered,x=\"Sample ID\", y=\"Individual residual variance\",hue='Group',width=2)\n",
    "\n",
    "g2 = g.axes\n",
    "g2.set(xticklabels=[],xticks=[]) \n",
    "g2.axhline(l2_mean,color='r') #plot mean\n",
    "g2.axhline(l2_mean +(l2_std*3),color='r') #plot 3 standard deviations away from the mean\n",
    "g2.margins(x=0.01) #increase space between first and last bar and the edges \n",
    "\n",
    "#Add labels for outliers\n",
    "g2.text(3,610,\"1027114\")\n",
    "g2.text(36,510,\"1347882\")\n",
    "g2.text(215,430,\"Mean + 3sd\")\n",
    "g2.text(240,190,\"Mean\")  \n",
    "\n",
    "#plt.savefig( 'Figures/individual_residual_variance.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hist = df_impute.mean(axis = 0)\n",
    "sns.histplot(l2_norm, bins = 40,color='#BABAE2',edgecolor=\"k\",legend=False) \n",
    "\n",
    "plt.xlabel('Individual residual variance',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) \n",
    "\n",
    "#plt.savefig( 'Figures/individual_residual_variance_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of outliers outside of mean plus/minus 3 standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier detection step - Z-score method\n",
    "#The outliers should be removed before scaling\n",
    "df_norm = pd.DataFrame(StandardScaler().fit_transform(df_impute),columns=df_impute.columns, index=df_impute.index)\n",
    "\n",
    "df_norm = df_norm.to_numpy()\n",
    "outlier_count = np.sum((df_norm > 3) | (df_norm < -3),axis=1)\n",
    "outlier_df = pd.DataFrame(outlier_count, index = meta_data.index, columns = ['Outlier number'])\n",
    "outlier_df = outlier_df.sort_values(['Outlier number'], ascending=[False])\n",
    "\n",
    "\n",
    "\n",
    "#I originally determined the outliers by taking the outliers that are 3 standard deviations above the mean,\n",
    "#however if you look at the histogram below this is not accurate because the distribution is not a NORMAL DISTRIBUTION!\n",
    "#therefore it's better to just plot a histogram or barplot and determine by eye\n",
    "display(outlier_df[:10]) #Outliers 3 std above the mean (first 3)\n",
    "\n",
    "outlier_df['Outlier number'].mean()  #3.8108\n",
    "outlier_df['Outlier number'].std()  #12.17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hist = df_impute.mean(axis = 0)\n",
    "sns.histplot(outlier_df, bins = 50,color='#BABAE2',edgecolor=\"k\",legend=False) \n",
    "\n",
    "plt.title('Outlier count',fontsize=16)\n",
    "plt.xlabel('Number of outliers',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) \n",
    "\n",
    "#plt.savefig( 'Figures/outlier_histogram.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying outliers using pair plot method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pair plots for first 4 principal components\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2','PC3', 'PC4'], index = df.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, meta_data], axis = 1)\n",
    "#display(df3)\n",
    "\n",
    "sns.pairplot(df3, hue='Group',height=1.8)   \n",
    "\n",
    "#plt.savefig( 'Figures/outlier_pairplot.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, can dentify outliers by looking at which points are at the extremes of the PC axes and checking with the PCA plot\n",
    "outliers_pc = df3.sort_values(['PC4'], ascending=[False])\n",
    "display(outliers_pc.iloc[:10,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising outliers with first 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for visualisation WITH FIRST 2 PRINCIPAL COMPONENTS AND OUTLIERS LABELLED\n",
    "features = df_impute.columns[:]\n",
    "x = df_impute.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=2) #two principal components for data visualisation\n",
    "principal_components = pca.fit_transform(x)\n",
    "\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'], index = df.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, meta_data], axis = 1)\n",
    "\n",
    "display(df3)\n",
    "\n",
    "#Subset outlier dataframe to plot outliers\n",
    "outlier_df = df3[df3.index.isin(outliers)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=df3, \n",
    "    hue='WHO_status', \n",
    "    hue_order = ['0', '1-2', '3-4','5-7'],\n",
    "    fit_reg=False, \n",
    "    legend=False,\n",
    "    scatter_kws={\"s\": 20}\n",
    "    )\n",
    "#Note: I don't use the seaborn legend but check it matches with the seaborn legend\n",
    "\n",
    "for index,outlier in (enumerate(outliers)):\n",
    "    plt.text(df3.PC1[outlier],df3.PC2[outlier], outlier)\n",
    "\n",
    "\n",
    "plt.title('PCA for outlier detection',fontsize=20)\n",
    "plt.xlabel('PC1 (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)',fontsize=15)\n",
    "plt.ylabel('PC2 (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)',fontsize=15)\n",
    "plt.legend(framealpha=1, frameon = 'True', title=\"WHO status\",title_fontsize='large', prop={'size': 14}, bbox_to_anchor=(1.04, 0.7)) \n",
    "#This has more information on the bbox_to_anchor coordinates: https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot\n",
    "\n",
    "#plt.savefig( 'Figures/PCA_protein_QC.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of outliers prior to scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outlier 1027114\n",
    "df_impute2 = df_impute.drop(1027114)\n",
    "meta_data = meta_data.drop(1027114)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise by column i.e. each pathway is normalised across all patients\n",
    "#mean of the observed values becomes 0 and the standard deviation is 1\n",
    "\n",
    "\n",
    "df_norm = pd.DataFrame(StandardScaler().fit_transform(df_impute2),columns=df_impute2.columns, index=df_impute2.index)\n",
    "\n",
    "print(df_norm.max().max())\n",
    "print(df_norm.min().min())\n",
    "print(df_norm.mean(axis = 0)) #mean of 0  \n",
    "print(df_norm.std(axis = 0)) #sd of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One feature has a lower mean than the others (superoxide dismutase 2, mitochondrial)\n",
    "mean = pd.DataFrame(df_norm.mean(axis = 0))\n",
    "mean[[0]].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_norm.mean(axis = 0)\n",
    "sns.histplot(df_hist, bins = 50,color='#79C99E',edgecolor=\"k\") \n",
    "\n",
    "#The mean value for each metabolite has been plotted\n",
    "plt.title('Mean protein distribution',fontsize=16)\n",
    "plt.xlabel('Protein expression (e-15)',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) ;\n",
    "\n",
    "#plt.savefig( 'Figures/mean_protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_norm_np = df_norm.to_numpy()\n",
    "\n",
    "df_hist = df_norm_np.flatten()\n",
    "sns.histplot(df_hist, bins = 45,color='#79C99E',edgecolor=\"k\") \n",
    "\n",
    "#The mean value for each metabolite has been plotted\n",
    "plt.title('Protein distribution',fontsize=16)\n",
    "plt.xlabel('Protein expression',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) ;\n",
    "\n",
    "#plt.savefig( 'Figures/protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out PCA\n",
    "\n",
    "features = df_norm.columns[:]\n",
    "x = df_norm.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])\n",
    "\n",
    "#Restore original index\n",
    "df2 = df2.set_index(df_norm.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, meta_data], axis = 1)\n",
    "\n",
    "#display(df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=df3, \n",
    "    hue='WHO_status', \n",
    "    hue_order = ['0', '1-2', '3-4','5-7'],\n",
    "    fit_reg=False, #don't draw line of best fit\n",
    "    legend=False,\n",
    "    scatter_kws={\"s\": 20}\n",
    "    )\n",
    "#Note: I don't use the seaborn legend but check it matches with the seaborn legend\n",
    "\n",
    "\n",
    "plt.title('PCA for proteins',fontsize=20)\n",
    "plt.xlabel('PC1 (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)',fontsize=15,)\n",
    "plt.ylabel('PC2 (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)',fontsize=15)\n",
    "plt.legend(framealpha=1, frameon = 'True', title=\"WHO status\",title_fontsize='large', prop={'size': 14}, bbox_to_anchor=(1.04, 0.7)) \n",
    "\n",
    "#plt.savefig( 'Figures/proteins_PCA.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add metadata to the end of the df\n",
    "df_final = pd.concat([df_norm, meta_data],axis=1) \n",
    "#df_final.to_csv('Data/Su_COVID_proteomics_processed_names.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert protein columns from names to UniProt IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert protein columns from names to Uniprot IDs \n",
    "\n",
    "#Read in pre-processed dataframe\n",
    "df = pd.read_csv('Data/Su_COVID_proteomics_processed_names.csv', index_col=0)\n",
    "\n",
    "#Read in the conversion table\n",
    "conversion_tab = pd.read_csv('Data/proteomics_metadata.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate dataframe into numerical data and meta data\n",
    "meta_data = df.iloc[:,-2:]\n",
    "df = df.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset conversion table to only the proteins present in the data\n",
    "protein_names = list(df.columns)\n",
    "conversion_tab2 = conversion_tab[conversion_tab['gene_description'].isin(protein_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_tab2 = conversion_tab2.sort_values(by='gene_description')\n",
    "\n",
    "#Check duplicates in the gene description column have the same Uniprot IDs (they do)\n",
    "conversion_tab2 = conversion_tab2.drop_duplicates(subset='uniprot')\n",
    "conversion_tab2 = conversion_tab2.reset_index(drop=True)\n",
    "\n",
    "from collections import Counter\n",
    "mylist = conversion_tab2.gene_description\n",
    "\n",
    "dups = [k for k,v in Counter(mylist).items() if v>1]\n",
    "print(dups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniprot = df.rename(columns=dict(zip(conversion_tab2[\"gene_description\"], conversion_tab2[\"uniprot\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add metadata to the end of the df\n",
    "df_final = pd.concat([df_uniprot, meta_data],axis=1) \n",
    "#df_final.to_csv('Data/Su_COVID_proteomics_processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Imperial_Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
