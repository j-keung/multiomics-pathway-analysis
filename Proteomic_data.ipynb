{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries \n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import sspa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy \n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('Data/Su_COVID_proteomics_processed.csv', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of proteins:\", len(df. columns[:-2]) )\n",
    "print(\"Number of samples:\", len(df. index))\n",
    "\n",
    "\n",
    "print(df['WHO_status'].value_counts()) \n",
    "print(df['Group'].value_counts())\n",
    "\n",
    "#return non-integer columns\n",
    "df.dtypes[df.dtypes != 'int64'][df.dtypes != 'float64']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data is standardised \n",
    "\n",
    "df_num  = df.iloc[:,:-2] #all rows, all columns apart from last two\n",
    "print(df_num.max().max())\n",
    "print(df_num.min().min())\n",
    "print(df_num.mean(axis = 0)) #mean of 0\n",
    "print(df_num.std(axis = 0)) #sd of 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_num.mean(axis = 0)\n",
    "sns.histplot(df_hist, bins = 50,color='#79C99E',edgecolor=\"k\") \n",
    "\n",
    "#The mean value for each protein has been plotted\n",
    "plt.title('Mean protein distribution',fontsize=16)\n",
    "plt.xlabel('Protein expression (e-15)',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) ;\n",
    "\n",
    "#plt.savefig( 'Figures/mean_protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_np = df_num.to_numpy()\n",
    "\n",
    "df_hist = df_num_np.flatten()\n",
    "sns.histplot(df_hist, bins = 45,color='#79C99E',edgecolor=\"k\") \n",
    "\n",
    "plt.title('Protein distribution',fontsize=16)\n",
    "plt.xlabel('Protein expression',fontsize=13)\n",
    "plt.ylabel('Count',fontsize=13) ;\n",
    "\n",
    "#plt.savefig( 'Figures/protein_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.iloc[:,-5:])\n",
    "df_heatmap = df.groupby('WHO_status').mean(numeric_only=True)\n",
    "\n",
    "g = sns.clustermap(\n",
    "    df_heatmap,metric='euclidean',\n",
    "      method =\"ward\",\n",
    "      row_cluster=False,\n",
    "      xticklabels=False,\n",
    "      cmap='RdBu_r',\n",
    "      figsize=(9,3),\n",
    "      dendrogram_ratio=0.2, \n",
    "      vmin=-2, \n",
    "      vmax=2) \n",
    "\n",
    "g2 = g.ax_heatmap\n",
    "g2.set_xlabel(\"Proteins\", fontsize = 15,labelpad=10) #labelpad increases the distance between the axis label and the heatmap\n",
    "g2.set_ylabel(\"WHO status\", fontsize = 15, labelpad=12) \n",
    "g2.set_yticklabels(g2.get_yticklabels(), rotation=0, fontsize=10)  #rotate the y-axis labels so that they are horizontal\n",
    "\n",
    "x0, _y0, _w, _h = g.cbar_pos\n",
    "g.ax_cbar.set_position([1.15, 0.28, 0.03, 0.35])\n",
    "g.cax.set_title(\"Protein expression\",pad=10, size=13) #pad: increase spacing slightly  \n",
    "g.cax.tick_params(labelsize=10) #change font of colourbar labels; \n",
    "\n",
    "#plt.savefig( 'Figures/protein_heatmap.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out PCA\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "features = df.columns[:-2]\n",
    "x = df.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])\n",
    "\n",
    "#Restore original index\n",
    "df2 = df2.set_index(df.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, df[['WHO_status']]], axis = 1)\n",
    "\n",
    "display(df3)\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=df3, \n",
    "    hue='WHO_status', \n",
    "    hue_order = ['0', '1-2', '3-4','5-7'],\n",
    "    fit_reg=False, #don't draw line of best fit\n",
    "    legend=False,\n",
    "    scatter_kws={\"s\": 20}\n",
    "    )\n",
    "#Note: I don't use the seaborn legend but check it matches with the seaborn legend\n",
    "\n",
    "\n",
    "plt.title('PCA for proteins',fontsize=20)\n",
    "plt.xlabel('PC1 (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)',fontsize=15,)\n",
    "plt.ylabel('PC2 (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)',fontsize=15)\n",
    "plt.legend(framealpha=1, frameon = 'True', title=\"WHO status\",title_fontsize='large', prop={'size': 14}, bbox_to_anchor=(1.04, 0.7)) \n",
    "#This has more information on the bbox_to_anchor coordinates: https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot\n",
    "\n",
    "#plt.savefig( 'Figures/proteins_PCA.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR DATA INTEGRATION STEP OR IF COMPARING BY CASE SEVERITY\n",
    "### Subset the data to common samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two different networks, one for the COVID cases 1-2 compared to COVID cases 3-7 <br>\n",
    "This is because there are only 18 samples in common between the metabolomic and proteomic datasets\n",
    "\n",
    "0       Common samples: 18           Metabolomic samples: 133        Proteomic samples: 123 <br>\n",
    "1-2       Common samples: 45          Metabolomic samples: 45        Proteomic samples: 48 <br>\n",
    "3-4       Common samples: 56          Metabolomic samples: 57        Proteomic samples: 59 <br>\n",
    "5-7       Common samples: 27          Metabolomic samples: 28        Proteomic samples: 28 <br>\n",
    "\n",
    "146 common samples overall,   128 cases (45 samples (WHO 1-2) vs 83 samples (WHO 3-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Data/Su_COVID_metabolomics_processed_ChEBI.csv', index_col=0)\n",
    "df2.index= df2.index.str.rstrip('-BL')  #remove 'BL' label from the cases (so I can match to proteomic data)\n",
    "\n",
    "list1 = list(df.index)\n",
    "list2 = list(df2.index)\n",
    "intersection = list(set(list1).intersection(list(set(list2))))  #set removes duplicates\n",
    "print(len(intersection))\n",
    "\n",
    "df = df[df.index.isin(intersection)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mild = (df[df[\"WHO_status\"] == '1-2']) #45 samples, no need to remove the metadata, since I do that in a later step\n",
    "df_severe = (df[(df[\"WHO_status\"] == '3-4') | (df[\"WHO_status\"] == '5-7')]) #83 samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single sample pathway analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in a file downloaded from https://reactome.org/download/current/UniProt2Reactome_All_Levels.txt\n",
    "reactome_pathways = sspa.process_reactome('Homo sapiens', infile = 'Data/UniProt2Reactome_all_Levels.txt', download_latest = False, filepath = None)\n",
    "kpca_scores = sspa.sspa_kpca(df.iloc[:,:-2], reactome_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove root pathways\n",
    "#Convert pathway ID to name\n",
    "root_path = pd.read_excel('Data/Root_pathways.xlsx', header=None)\n",
    "root_pathway_dict = {root_path[0][i]:root_path[1][i] for i in range(0,len(root_path))}\n",
    "\n",
    "root_pathway_names = list(root_pathway_dict.keys())\n",
    "#Using Sara's code\n",
    "kpca_scores = kpca_scores.drop(columns = list(set(root_pathway_names) & set(kpca_scores.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kPCA figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kpca_hist = kpca_scores.to_numpy()\n",
    "kpca_hist = kpca_hist.flatten()\n",
    "sns.histplot(kpca_hist, bins = 40,color='#F7C3B1',edgecolor=\"k\") \n",
    "\n",
    "plt.title('Protein pathway score distribution',fontsize=16)\n",
    "plt.xlabel('Pathway score',fontsize=13, labelpad=5)\n",
    "plt.ylabel('Frequency',fontsize=13, labelpad=10) ;\n",
    "\n",
    "#plt.savefig( 'Figures/protein_pathway_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out PCA\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "features = kpca_scores.columns\n",
    "x = kpca_scores.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])\n",
    "\n",
    "#Restore original index\n",
    "df2 = df2.set_index(df.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, df[['WHO_status']]], axis = 1)\n",
    "\n",
    "#display(df3)\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=df3, \n",
    "    hue='WHO_status', \n",
    "    hue_order = ['0', '1-2', '3-4','5-7'],\n",
    "    fit_reg=False, \n",
    "    legend=False,\n",
    "    scatter_kws={\"s\": 20}\n",
    "    )\n",
    "#Note: I don't use the seaborn legend but check it matches with the seaborn legend\n",
    "\n",
    "\n",
    "plt.title('PCA kPCA protein scores',fontsize=20)\n",
    "plt.xlabel('PC1 (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)',fontsize=15)\n",
    "plt.ylabel('PC2 (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)',fontsize=15)\n",
    "plt.legend(framealpha=1, frameon = 'True', title=\"WHO status\",title_fontsize='large', prop={'size': 14}, bbox_to_anchor=(1.04, 0.7))\n",
    "#This has more information on the bbox_to_anchor coordinates: https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot\n",
    "#plt.savefig( 'Figures/protein_kPCA_PCA.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise pathway scores : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "kpca_scores_norm = pd.DataFrame(StandardScaler().fit_transform(kpca_scores),columns=kpca_scores.columns, index=kpca_scores.index)\n",
    "\n",
    "\n",
    "kpca_hist = kpca_scores_norm.to_numpy()\n",
    "kpca_hist = kpca_hist.flatten()\n",
    "sns.histplot(kpca_hist, bins = 40,color='#F7C3B1',edgecolor=\"k\") \n",
    "\n",
    "plt.title('Normalised protein pathway score distribution',fontsize=16)\n",
    "plt.xlabel('Pathway score',fontsize=13, labelpad=5)\n",
    "plt.ylabel('Frequency',fontsize=13, labelpad=10) ;\n",
    "\n",
    "#plt.savefig( 'Figures/normalised_protein_pathway_distribution.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out PCA on NORMALISED values\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "features = kpca_scores_norm.columns\n",
    "x = kpca_scores_norm.loc[:, features].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(x)\n",
    "df2 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])\n",
    "\n",
    "#Restore original index\n",
    "df2 = df2.set_index(df.index)\n",
    "\n",
    "#Concatenate WHO information\n",
    "df3 = pd.concat([df2, df[['WHO_status']]], axis = 1)\n",
    "\n",
    "#display(df3)\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=df3, \n",
    "    hue='WHO_status', \n",
    "    hue_order = ['0', '1-2', '3-4','5-7'],\n",
    "    fit_reg=False, \n",
    "    legend=False,\n",
    "    scatter_kws={\"s\": 20}\n",
    "    )\n",
    "#Note: I don't use the seaborn legend but check it matches with the seaborn legend\n",
    "\n",
    "\n",
    "plt.title('PCA kPCA normalised protein scores',fontsize=20)\n",
    "plt.xlabel('PC1 (' + str(round(pca.explained_variance_ratio_[0]*100,2)) + '%)',fontsize=15)\n",
    "plt.ylabel('PC2 (' + str(round(pca.explained_variance_ratio_[1]*100,2)) + '%)',fontsize=15)\n",
    "plt.legend(framealpha=1, frameon = 'True', title=\"WHO status\",title_fontsize='large', prop={'size': 14}, bbox_to_anchor=(1.04, 0.7))\n",
    "#This has more information on the bbox_to_anchor coordinates: https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot\n",
    "#plt.savefig( 'Figures/normalised_protein_kPCA_PCA.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spearman correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Spearman correlation coefficient results are the same whether or not the kPCA scores are normalised\n",
    "\n",
    "#\"If axis=0 (default), then each column represents a variable, with observations in the rows\"\n",
    "spearman_results = scipy.stats.spearmanr(kpca_scores)\n",
    "\n",
    "spearman_coef = spearman_results[0] #correlation coefficients\n",
    "spearman_pvals = spearman_results[1] #p-values\n",
    "\n",
    "\n",
    "#Using Sara's code (rather than having separate dataframes for each analysis, add all together in long format)\n",
    "squared_spearman_coef_df = pd.DataFrame(spearman_coef,columns = kpca_scores.columns, index=kpca_scores.columns)\n",
    "squared_spearman_coef_list = squared_spearman_coef_df.stack().reset_index()\n",
    "squared_spearman_coef_list.columns = [\"Pathway1\", \"Pathway2\", \"Spearman_corr\"]\n",
    "squared_spearman_coef_list[\"Squared_corr\"]  = np.square(squared_spearman_coef_list.Spearman_corr)\n",
    "\n",
    "spearman_pvals_df = pd.DataFrame(spearman_pvals,columns = kpca_scores.columns, index=kpca_scores.columns)\n",
    "spearman_pvals_list = spearman_pvals_df.stack().reset_index()\n",
    "spearman_pvals_list.columns = [\"Pathway1\", \"Pathway2\", \"pval\"]\n",
    "\n",
    "#Multiple testing correction for the p-values to prepare the corrected p-values for the final correlation network\n",
    "#Multiplies by the correct number of tests (i.e. not including the duplicates or self-comparisons)\n",
    "#Does not remove the diagonals or the duplicates themselves\n",
    "num_of_tests = (len(kpca_scores.columns)**2 - len(kpca_scores.columns))/2\n",
    "print(num_of_tests)\n",
    "corrected_spearman_pvals = spearman_pvals_list.pval*num_of_tests\n",
    "#If the p-val goes beyond 1 (max number for a p-value, change to 1)\n",
    "corrected_spearman_pvals = np.where(corrected_spearman_pvals < 1, corrected_spearman_pvals, 1)\n",
    "spearman_pvals_list[\"pval_adj\"]  = corrected_spearman_pvals\n",
    "\n",
    "spearman_df = squared_spearman_coef_list.merge(spearman_pvals_list,on=[\"Pathway1\",\"Pathway2\"])\n",
    "\n",
    "display(spearman_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform multiple testing correlation on p-values after the duplicates are removed\n",
    "\n",
    "#https://tedboy.github.io/statsmodels_doc/generated/statsmodels.stats.multitest.multipletests.html#statsmodels.stats.multitest.multipletests\n",
    "\n",
    "#All self-comparisons are significant with a p-value of 0, so we can subtract those from the number of significant values before we divide by 2\n",
    "sig_vals = (sum(i < 0.005 for i in spearman_df.pval_adj)-len(kpca_scores.columns))   /2\n",
    "non_sig_vals = sum(i >= 0.005 for i in spearman_df.pval_adj)/2\n",
    "\n",
    "print(\"Number of significant values:\", sig_vals)\n",
    "print(\"Number of non-significant values:\", non_sig_vals)\n",
    "(sig_vals/(sig_vals+non_sig_vals)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_spearman_coef = np.square(spearman_coef)\n",
    "abs_spearman_coef = np.abs(spearman_coef)\n",
    "\n",
    "#Mask the upper half of the dataframe (so I don't view the comparisons between the two same genes, and also the duplicate comparisons are removed)\n",
    "mask =  squared_spearman_coef.copy()\n",
    "mask = np.triu(np.ones(mask.shape)).astype(bool)\n",
    "mask = np.invert(mask) #invert true and false values so the diagonal is False as well\n",
    "non_dup_squared_spearman_coef = pd.DataFrame(squared_spearman_coef)\n",
    "non_dup_squared_spearman_coef = non_dup_squared_spearman_coef.where(mask) #Replace all false values with NaN using mask\n",
    "#display(non_dup_squared_spearman_coef)\n",
    "#display(pd.DataFrame(squared_spearman_coef))\n",
    "\n",
    "spearman_hist = non_dup_squared_spearman_coef.to_numpy().flatten()\n",
    "spearman_hist = spearman_hist[~np.isnan(spearman_hist)] #remove nan values\n",
    "print(len(spearman_hist))\n",
    "#spearman_hist = squared_spearman_coef.to_numpy().flatten()\n",
    "sns.histplot(spearman_hist, bins = 50,color='#FFD580',edgecolor=\"k\") \n",
    "\n",
    "plt.title('Spearman correlation coefficient distribution',fontsize=16)\n",
    "plt.xlabel('Correlation score',fontsize=16, labelpad=5)\n",
    "plt.ylabel('Frequency',fontsize=16, labelpad=10) ;\n",
    "\n",
    "#plt.savefig('Figures/protein_spearman_correlation_distribution_squared_non_dup.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The reason for plotting as absolute or squared is that kPCA does not determine the directionality of effect\n",
    "#Therefore the direction specified here is arbitrary i.e. negative pathway score does not mean downregulation of pathway\n",
    "\n",
    "\n",
    "g = sns.clustermap(\n",
    "    squared_spearman_coef,\n",
    "    metric='euclidean', \n",
    "    method =\"ward\",\n",
    "    cmap=\"OrRd\",\n",
    "    xticklabels=False, \n",
    "    yticklabels=False,\n",
    "    figsize=(6,6),\n",
    "    dendrogram_ratio=0.15, \n",
    "    vmin=0, \n",
    "    vmax=1) \n",
    "\n",
    "g2 = g.ax_heatmap\n",
    "g2.set_xlabel(\"Pathways\", fontsize = 17, labelpad=10) #labelpad increases the distance between the axis label and the heatmap\n",
    "g2.set_ylabel(\"Pathways\", fontsize = 17, labelpad=10) \n",
    "\n",
    "x0, _y0, _w, _h = g.cbar_pos\n",
    "g.ax_cbar.set_position([1.1, 0.25, 0.03, 0.35])\n",
    "g.cax.set_title(\"Correlation score\",pad=13,size=13) #pad: increase spacing slightly  \n",
    "g.cax.tick_params(labelsize=12) #change font of colourbar labels; \n",
    "\n",
    "#plt.savefig( 'Figures/squared_protein_spearman.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Obtain pathways and corresponding proteins, store as dictionary\n",
    "orig_dict = sspa.utils.pathwaydf_to_dict(reactome_pathways)\n",
    "\n",
    "#Filter out dictionary to retain only the pathways that remain after kPCA\n",
    "my_keys = kpca_scores.columns\n",
    "pathways_dict = {key: orig_dict[key] for key in my_keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the compounds in the pathways that are not present in the dataset\n",
    "\n",
    "#Obtain all unique values in dataset\n",
    "compounds_present = list(df.columns[:-2])\n",
    "filtered_dict = {} \n",
    "\n",
    "#My code adapted from Cecilia's\n",
    "#If the key values are not part of the compounds in dataset then remove\n",
    "for key,value in pathways_dict.items():\n",
    "    new_val = [item for item in value if item in compounds_present]\n",
    "    if len(new_val) >= 2: #at least two compounds in the pathway\n",
    "        filtered_dict[key] = new_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Identifying all duplicate pathways from the Reactome pathway dictionary AFTER COMPOUNDS NOT IN PATHWAY REMOVED (the exact same proteins, not subsets)\n",
    "\n",
    "proteins = list(filtered_dict.values())\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter(map(tuple,proteins))\n",
    "dups = [k for k,v in c.items() if v>1]\n",
    "result = [list(t) for t in dups]\n",
    "\n",
    "counter = 0\n",
    "for j in result:\n",
    "    value = {i for i in filtered_dict if filtered_dict[i]==j}\n",
    "    if len(value)!=0:\n",
    "        counter +=1\n",
    "    print(value)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the pathways with the minimum and maximum number of proteins in pathways AFTER COMPOUNDS NOT IN PATHWAY REMOVED\n",
    "\n",
    "max_len = max(proteins, key=len)\n",
    "print(len(max_len))\n",
    "\n",
    "min_len = min(proteins, key=len)\n",
    "print(len(min_len))\n",
    "\n",
    "\n",
    "for index in range(0,len(proteins)):\n",
    "    length =  90\n",
    "    value = {key for key in filtered_dict if len(filtered_dict[key])==length}\n",
    "print(value)\n",
    "\n",
    "#len(filtered_dict['R-HSA-168256'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference between Jaccard similarity metric and S-S Overlap Coefficient\n",
    "#https://developer.nvidia.com/blog/similarity-in-graphs-jaccard-versus-the-overlap-coefficient/\n",
    "\n",
    "#Using Overlap Coefficient formula \n",
    "#My original code is in 'Overlap_coefficient_ver83.ipynb'\n",
    "#I adapted by code by looking at Cecilia's comments\n",
    "oc_matrix = np.zeros((len(my_keys),len(my_keys)))    \n",
    "\n",
    "for i in range(0,len(my_keys)):   \n",
    "    list1 = filtered_dict[my_keys[i]]\n",
    "    \n",
    "    for j in range(0,len(my_keys)):\n",
    "        list2 = filtered_dict[my_keys[j]]\n",
    "\n",
    "        # Szymkiewiczâ€“Simpson coefficient\n",
    "        #Find intersection between two lists\n",
    "        intersection = len(list(set(list1).intersection(list(set(list2)))))\n",
    "        smaller_set = min(len(list1), len(list2))\n",
    "\n",
    "        val = intersection/smaller_set\n",
    "        oc_matrix[i][j] = val \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_df = pd.DataFrame(oc_matrix, index=filtered_dict.keys(), columns=filtered_dict.keys())\n",
    "#Calculate number of edges with values above or equal to 0.5\n",
    "\n",
    "#If looking at whole matrix (not accounting for self comparisons or duplicates)\n",
    "#print(np.count_nonzero(oc_matrix >= 0.5)/(len(my_keys)*len(my_keys)))\n",
    "\n",
    "#Subtract by number of self-comparisons divided by 2\n",
    "high_overlap = (np.count_nonzero(oc_matrix >= 0.5) - len(my_keys)) / 2\n",
    "unique_edges =  ((len(my_keys)*len(my_keys)) - len(my_keys)) / 2\n",
    "\n",
    "print(high_overlap  /  unique_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_list = oc_df.stack().reset_index()\n",
    "oc_list.columns = [\"Pathway1\", \"Pathway2\", \"Overlap_coef\"]\n",
    "spearman_df = spearman_df.merge(oc_list,on=[\"Pathway1\",\"Pathway2\"])\n",
    "\n",
    "display(spearman_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(oc_matrix,\n",
    "metric='euclidean', \n",
    "method =\"ward\",\n",
    "cmap = \"OrRd\", \n",
    "xticklabels=False, \n",
    "yticklabels=False, \n",
    "figsize=(6,6),\n",
    "dendrogram_ratio=0.2, \n",
    "vmin=-0, \n",
    "vmax=1) \n",
    "\n",
    "g2 = g.ax_heatmap\n",
    "g2.set_xlabel(\"Pathways\", fontsize = 17, labelpad=10) #labelpad increases the distance between the axis label and the heatmap\n",
    "g2.set_ylabel(\"Pathways\", fontsize = 17, labelpad=10) \n",
    "g2.set(yticklabels=[])  #remove tick labels\n",
    "g2.tick_params(right=False) #remove ticks\n",
    "\n",
    "x0, _y0, _w, _h = g.cbar_pos\n",
    "g.ax_cbar.set_position([1.15, 0.25, 0.03, 0.35])\n",
    "g.cax.set_title(\"Overlap Coefficient\",pad=13,size=13) #pad: increase spacing slightly  \n",
    "g.cax.tick_params(labelsize=12) #change font size of colourbar labels; \n",
    "\n",
    "#plt.savefig( 'Figures/protein_overlap_coef.png' , dpi=200,bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing overlap coefficient network graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove self-correlations\n",
    "spearman_df = spearman_df [spearman_df.Pathway1 != spearman_df.Pathway2]\n",
    "spearman_df = spearman_df.reset_index(drop=True)\n",
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Add the nodes\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(oc_df.columns)\n",
    "\n",
    "#does not deal with duplicates i.e.  Pathway 1 to Pathway 2 and vice versa\n",
    "#but it's ok, because it shows up as one edge on the network in Cytoscape\n",
    "\n",
    "G = nx.from_pandas_edgelist(df=spearman_df, source='Pathway1', target='Pathway2', edge_attr='Overlap_coef')\n",
    "\n",
    "nx.draw(G, with_labels = True)\n",
    "print(G.number_of_edges())\n",
    "print(G.number_of_nodes())\n",
    "#nx.write_gml(G, \"Cytoscape/proteomic_oc.gml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final correlation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE A GRAPH WITH ALL THE EDGES FOR THE INTEGRATION STAGE\n",
    "\n",
    "G = nx.Graph()\n",
    "G = nx.from_pandas_edgelist(df=spearman_df, source='Pathway1', target='Pathway2', edge_attr='Squared_corr')\n",
    "#G.add_nodes_from(isolated_nodes)\n",
    "nx.draw(G, with_labels = True)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "\n",
    "#Add edge attributes\n",
    "spearman_pval_dict = {}\n",
    "overlap_coef_dict = {}\n",
    "for i in range(0,len(spearman_df)):\n",
    "    spearman_pval_dict[(spearman_df.Pathway1[i],spearman_df.Pathway2[i])] = spearman_df.pval_adj[i]\n",
    "    overlap_coef_dict[(spearman_df.Pathway1[i],spearman_df.Pathway2[i])] = spearman_df.Overlap_coef[i]\n",
    "\n",
    "\n",
    "nx.set_edge_attributes(G, spearman_pval_dict, \"Spearman_pval\")\n",
    "nx.set_edge_attributes(G, overlap_coef_dict, \"Overlap_coef\")\n",
    "\n",
    "\t\t\n",
    "G.edges[\"R-HSA-109581\",\"R-HSA-109704\"]#[\"Spearman_pval\"]\n",
    "\n",
    "#nx.write_gml(G,'Cytoscape/proteomic_prefiltered.gml')\n",
    "#nx.write_gml(G,'Cytoscape/proteomic_prefiltered_severecases.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create EDGE ATTRIBUTES table to filter out unneeded edges\n",
    "\n",
    "final_df = spearman_df[spearman_df[\"pval_adj\"] < 0.005]  \n",
    "final_df = final_df[final_df[\"Overlap_coef\"] < 0.5]\n",
    "final_df = final_df.reset_index(drop=True) \n",
    "display(final_df) #the duplicate edges have not been removed yet\n",
    "\n",
    "#In some instances, I have nodes which have no edges, and the results are different to Sara's\n",
    "#Can check the pathways prior to Overlap Coefficient filtering here\n",
    "#final_df[final_df[\"Pathway1\"] == \"R-HSA-109581\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot spearman correlation histogram to show edges AFTER filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates in the edge list\n",
    "\n",
    "final_df_copy = final_df.copy()\n",
    "\n",
    "#Remove duplicate pathways\n",
    "for i in range(0,len(final_df)):\n",
    "    val1 = final_df.Pathway1[i]\n",
    "    val2 = final_df.Pathway2[i]\n",
    "    #print(val1,val2)\n",
    "    #print(max(val1,val2))\n",
    "    final_df_copy.Pathway1[i] = min(val1,val2)\n",
    "    final_df_copy.Pathway2[i] = max(val1,val2)\n",
    "\n",
    "final_df_copy = final_df_copy.sort_values(['Pathway1','Pathway2'], ascending=True)\n",
    "final_df_copy = final_df_copy[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_df_copy))\n",
    "spearman_hist = list(final_df_copy.Squared_corr)\n",
    "sns.histplot(spearman_hist, bins = 50,color='#FFD580',edgecolor=\"k\") \n",
    "\n",
    "plt.title('Spearman correlation coefficient distribution',fontsize=16)\n",
    "plt.xlabel('Correlation score',fontsize=16, labelpad=5)\n",
    "plt.ylabel('Frequency',fontsize=16, labelpad=10) ;\n",
    "\n",
    "#plt.savefig('Figures/protein_spearman_correlation_distribution_squared_afterfiltering.png' , dpi=200, bbox_inches = 'tight' , pad_inches = 0.2 , facecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathways_with_edges = set(final_df.Pathway1)\n",
    "isolated_nodes = set(spearman_df.Pathway1) - set(pathways_with_edges) \n",
    "isolated_nodes\n",
    "#len(pathways_with_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw network graph with new edges\n",
    "G = nx.Graph()\n",
    "G = nx.from_pandas_edgelist(df=final_df, source='Pathway1', target='Pathway2', edge_attr='Squared_corr')\n",
    "#G.add_nodes_from(isolated_nodes)\n",
    "nx.draw(G, with_labels = True)\n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add edge attributes\n",
    "spearman_pval_dict = {}\n",
    "overlap_coef_dict = {}\n",
    "for i in range(0,len(final_df)):\n",
    "    spearman_pval_dict[(final_df.Pathway1[i],final_df.Pathway2[i])] = final_df.pval_adj[i]\n",
    "    overlap_coef_dict[(final_df.Pathway1[i],final_df.Pathway2[i])] = final_df.Overlap_coef[i]\n",
    "    \n",
    "nx.set_edge_attributes(G, spearman_pval_dict, \"Spearman_pval\")\n",
    "nx.set_edge_attributes(G, overlap_coef_dict, \"Overlap_coef\")\n",
    "\n",
    "#G.edges[\"R-HSA-109581\",\"R-HSA-109704\"]#[\"Spearman_pval\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF YOU DON'T REMOVE THE EDGES, DON'T RUN THIS CODE\n",
    "\n",
    "#Add the betweenness centrality as a node attribute\n",
    "betweenness= nx.betweenness_centrality(G, normalized=True) #output as dictionary\n",
    "#display(betweenness)\n",
    "nx.set_node_attributes(G, betweenness, \"betweenness\")\n",
    "\n",
    "#Add the betweenness centrality as a node attribute\n",
    "degree= nx.degree_centrality(G) #output as dictionary\n",
    "#display(degree)\n",
    "nx.set_node_attributes(G, degree, \"degrees\")\n",
    "\n",
    "\n",
    "#Check\n",
    "G.nodes['R-HSA-983705']#[\"betweenness\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the highest level of the pathway hierarchy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Cecilia's code\n",
    "\n",
    "#Creating graph\n",
    "hierarchy = pd.read_csv('Data/ReactomePathwaysRelation.txt', sep='\\t', header=None)\n",
    "\n",
    "#From the pathways, subset to Homo sapiens only\n",
    "hierarchy_hsa = hierarchy[hierarchy[0].str.contains('HSA')]\n",
    "\n",
    "#Return unique values in the first column that is not in the second column as a numpy array\n",
    "#These values are not child pathways in any instances\n",
    "hierarchy_hsa_parents = np.setdiff1d(hierarchy_hsa[0], hierarchy_hsa[1])\n",
    "\n",
    "#Add the unique values not in the second column as a second attached dataset to the bottom of the original data\n",
    "#The first column represents the parent column, the second column is the child column\n",
    "hierarchy_hsa_all = pd.concat([hierarchy_hsa, pd.DataFrame([hierarchy_hsa_parents, hierarchy_hsa_parents], index=[0, 1]).T])\n",
    "\n",
    "#DiGraph is a directed graph\n",
    "H = nx.from_pandas_edgelist(hierarchy_hsa, source=0, target=1, create_using=nx.DiGraph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the root pathway\n",
    "\n",
    "def find_root(H,child):\n",
    "    #Find parent from child \n",
    "    parent = list(H.predecessors(child))\n",
    "\n",
    "    #Keep the loop going until the highest level is reached\n",
    "    if len(parent) == 0:\n",
    "        return child\n",
    "    else:  \n",
    "        return find_root(H, parent[0])\n",
    "\n",
    "hierarchy_hsa_all['Root'] = [find_root(H, i) for i in hierarchy_hsa_all[1]]\n",
    "\n",
    "hierarchy_hsa_all.columns = ['Parent', 'Child', 'Root']\n",
    "\n",
    "#There are 83 instances of duplicates, however all the child duplicates have the same root (even though different parents) after checking\n",
    "\n",
    "root_pathways = {}\n",
    "for pathway in list(H.nodes):\n",
    "    index = hierarchy_hsa_all.Child[hierarchy_hsa_all.Child == pathway].index.tolist()[0]\n",
    "    root_pathway  = hierarchy_hsa_all.Root[index]\n",
    "    label = root_pathway_dict[root_pathway]\n",
    "    root_pathways[pathway] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows all the root pathways in Reactome\n",
    "set(hierarchy_hsa_all['Root'] )\n",
    "#shows all the root pathways present in the original dataset\n",
    "set(root_pathways.values())\n",
    "\n",
    "nx.set_node_attributes(G, root_pathways, \"root_pathway\")\n",
    "\n",
    "print(G.number_of_nodes()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_clusters = nx.community.louvain_communities(G, weight='Spearman_coef',seed=100,resolution=1.005)\n",
    "print(len(louvain_clusters))\n",
    "\n",
    "louvain_dict = {}\n",
    "for index,grouping in enumerate(louvain_clusters):\n",
    "    for pathway in grouping:\n",
    "        louvain_dict[pathway] = index+1\n",
    "\n",
    "nx.set_node_attributes(G, louvain_dict, \"louvain\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactome_pathways = sspa.process_reactome('Homo sapiens', infile = 'Data/UniProt2Reactome_all_Levels.txt', download_latest = False, filepath = None)\n",
    "\n",
    "pathway_name_dict = {reactome_pathways.index[i]:reactome_pathways[\"Pathway_name\"][i] for i in range(0,len(reactome_pathways))}\n",
    "pathway_name_dict = {k:pathway_name_dict[k] for k in list(G.nodes)}\n",
    "\n",
    "nx.set_node_attributes(G, pathway_name_dict, \"pathway_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "\n",
    "print(G.edges[\"R-HSA-109581\",\"R-HSA-109704\"])#[\"Spearman_pval\"]\n",
    "print(G.nodes['R-HSA-109581'])#[\"betweenness\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_gml(G,'Cytoscape/proteomic_final.gml')\n",
    "#nx.write_gml(G,'Cytoscape/proteomic_final_severecases.gml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Imperial_Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
